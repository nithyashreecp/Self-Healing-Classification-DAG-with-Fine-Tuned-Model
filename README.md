## Project Title

**Self-Healing Text Classification DAG with Fine-Tuned Transformer**

## 1. Project Overview

This project builds a **self-healing text classification pipeline** using a LangGraph DAG. The pipeline fine-tunes a transformer model (DistilBERT) on sentiment analysis and incorporates a **fallback mechanism** for low-confidence predictions. The goal is to ensure robustness and correctness in human-in-the-loop workflows.

Key features:

* Fine-tuned transformer for text classification
* Confidence-based self-healing mechanism
* Multiple fallback strategies:

  * Ask user for clarification
  * Zero-shot backup classifier
  * Combined `ask_then_zero_shot` approach
* Structured logging for predictions, fallbacks, and final decisions
* CLI interface for interactive testing

---

## 2. Dataset

* **Dataset used:** IMDb movie reviews (binary sentiment: Positive / Negative)
* **Source:** HuggingFace `datasets` library
* **Size:** 25,000 training samples, 25,000 test samples

---

## 3. Folder Structure

```
atg-self-healing/
├── nodes/
│   ├── inference_node.py
│   ├── confidence_node.py
│   ├── fallback_node.py
│   └── dag.py
├── train.py                # Fine-tune transformer
├── cli.py                  # Command-line interface
├── demo_logs.log           # Generated by JSONLogger
├── saved_model/            # Fine-tuned model + tokenizer
├── visualize_stats.py      # Optional script for confidence/fallback stats
└── README.md
```

---

## 4. Installation

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate     # Windows

# Install dependencies
pip install torch transformers datasets
```

Optional for zero-shot fallback:

```bash
pip install transformers[sentencepiece]
```

---

## 5. Fine-Tuning the Model

```bash
python train.py
```

* Fine-tunes **DistilBERT** on IMDb dataset
* Saves model & tokenizer in `saved_model/`
* Default hyperparameters:

  * 1 epoch
  * Batch size: 8
  * Max sequence length: 128

---

## 6. Running the CLI

```bash
python cli.py --model_dir saved_model --threshold 0.70 --fallback ask_then_zero_shot
```

* `--model_dir`: path to saved model
* `--threshold`: confidence threshold (0–1)
* `--fallback`: fallback strategy (`ask_user`, `zero_shot`, `ask_then_zero_shot`)

**Example CLI interaction:**

```
Input: The movie was painfully slow and boring.
[InferenceNode] Predicted label: NEGATIVE | Confidence: 99.5%
[ConfidenceCheckNode] Confidence (99.5%) >= threshold (70%). Accepting prediction.
Final Label: NEGATIVE (Accepted automatically)

Input: The movie was okay, but it dragged a bit in the middle.
[InferenceNode] Predicted label: NEGATIVE | Confidence: 86.0%
[ConfidenceCheckNode] Confidence (86.0%) < threshold (90%). Triggering fallback...
[FallbackNode] Could you clarify your intent?
User: yes it is positive
Final Label: POSITIVE (Corrected via fallback)
```

---

## 7. LangGraph DAG Design

**Nodes:**

1. **InferenceNode** – Uses fine-tuned model for prediction; outputs label + confidence
2. **ConfidenceCheckNode** – Checks confidence against threshold; triggers fallback if low
3. **FallbackNode** – Handles:

   * Asking user clarification
   * Zero-shot fallback using `facebook/bart-large-mnli`
4. **JSONLogger** – Logs all events: predictions, confidence, fallback, final decisions

**Workflow:**
Input → InferenceNode → ConfidenceCheckNode → (FallbackNode if needed) → Final Label

---

## 8. Logging

* All predictions and fallback actions logged in `demo_logs.log` (JSON format)
* Sample log entries:

```json
{
  "timestamp": "2025-10-17T12:45:01",
  "node": "inference",
  "data": {
    "input_text": "The movie was okay",
    "predicted_label": "NEGATIVE",
    "confidence": 0.86
  }
}
```

---

## 9. Optional: Confidence & Fallback Stats

You can generate **confidence curves** and **fallback frequency charts** using `visualize_stats.py`:

```python
# visualize_stats.py
import json
from collections import Counter
import matplotlib.pyplot as plt

log_file = "demo_logs.log"
confidences = []
fallbacks = 0

with open(log_file) as f:
    for line in f:
        entry = json.loads(line)
        if entry["node"] == "inference":
            confidences.append(entry["data"]["confidence"])
        if entry["node"] == "fallback_result":
            fallbacks += 1

plt.hist(confidences, bins=20)
plt.title("Model Prediction Confidence Distribution")
plt.xlabel("Confidence")
plt.ylabel("Frequency")
plt.show()

print(f"Total fallbacks triggered: {fallbacks}")
```

